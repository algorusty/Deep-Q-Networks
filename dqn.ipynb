{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:39<00:00, 13.02s/it]\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 56.0kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 3.07MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 4.38MB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<?, ?B/s] \n",
      "model.safetensors: 100%|██████████| 440M/440M [00:15<00:00, 28.9MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"fblgit/juanako-7b-UNA\"\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # device_map=\"cuda:0\"\n",
    ")\n",
    "# Create the tokenizer from the model object\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# print(llm(\"AI is going to\"))\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 9368, Reward: 0.546033063525356, Total Reward: 0.546033063525356\n",
      "Action: 18911, Reward: 0.5460111141347814, Total Reward: 1.0920441776601373\n",
      "Action: 17062, Reward: 0.5461471952474567, Total Reward: 1.638191372907594\n",
      "Action: 28018, Reward: 0.5461201148335372, Total Reward: 2.184311487741131\n",
      "Action: 1723, Reward: 0.5461813703082143, Total Reward: 2.7304928580493453\n",
      "Action: 19920, Reward: 0.5961829883210834, Total Reward: 3.3266758463704287\n",
      "Action: 23681, Reward: 0.5962133556105194, Total Reward: 3.922889201980948\n",
      "Action: 31967, Reward: 0.5963372673356294, Total Reward: 4.519226469316577\n",
      "Action: 14225, Reward: 0.5963166082484389, Total Reward: 5.115543077565016\n",
      "Action: 27586, Reward: 0.5963520673443402, Total Reward: 5.711895144909357\n",
      "Action: 8327, Reward: 0.5963219674565747, Total Reward: 6.308217112365932\n",
      "Action: 10419, Reward: 0.5963333948154226, Total Reward: 6.904550507181354\n",
      "Action: 21148, Reward: 0.6463273319387088, Total Reward: 7.550877839120063\n",
      "Action: 10023, Reward: 0.6462834638767164, Total Reward: 8.19716130299678\n",
      "Action: 21412, Reward: 0.6462999289412199, Total Reward: 8.843461231937999\n",
      "Action: 30757, Reward: 0.6463105675742958, Total Reward: 9.489771799512296\n",
      "Action: 29668, Reward: 0.6462482386490456, Total Reward: 10.136020038161341\n",
      "Action: 16425, Reward: 0.6463045815554519, Total Reward: 10.782324619716793\n",
      "Action: 25129, Reward: 0.646286215992164, Total Reward: 11.428610835708957\n",
      "Action: 27553, Reward: 0.6463155384167288, Total Reward: 12.074926374125686\n",
      "Action: 12152, Reward: 0.6463121610140372, Total Reward: 12.721238535139724\n",
      "Action: 24606, Reward: 0.6463188421997575, Total Reward: 13.367557377339482\n",
      "Action: 12094, Reward: 0.6463070158624866, Total Reward: 14.013864393201969\n",
      "Action: 13637, Reward: 0.5963330506633543, Total Reward: 14.610197443865323\n",
      "Action: 20235, Reward: 0.5962884987707626, Total Reward: 15.206485942636085\n",
      "Action: 23812, Reward: 0.5963167697327781, Total Reward: 15.802802712368864\n",
      "Action: 30218, Reward: 0.5963165024188959, Total Reward: 16.39911921478776\n",
      "Action: 14435, Reward: 0.596321435100313, Total Reward: 16.995440649888074\n",
      "Action: 3854, Reward: 0.5963305447580425, Total Reward: 17.591771194646117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Network forward: Input shape:\", x.shape)  # Should be [1, 512]\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate, gamma):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQNNetwork(state_size, action_size, hidden_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # print(\"select_action: Original shape of state:\", state.shape)\n",
    "        state = state.float()\n",
    "        state_tensor = pad_or_truncate(state, max_length=512)\n",
    "        # # If state is a 2D tensor of shape [1, 512], no need to average across the last dimension\n",
    "        # if state.ndim == 2 and state.shape[1] == 512:\n",
    "        #     state_tensor = state\n",
    "        # elif state.ndim == 2:\n",
    "        #     # If state is a 2D tensor but not of the correct size, pad or truncate\n",
    "        #     state_tensor = pad_or_truncate(state, max_length=512)\n",
    "        # else:\n",
    "        #     # If state is not a 2D tensor, compute the embeddings\n",
    "        #     state_tensor = pad_or_truncate(state, max_length=512)\n",
    "        state_tensor = state_tensor.unsqueeze(0)  # Ensure it has a batch dimension\n",
    "        # print(\"select_action: Shape of state_tensor before passing to the network:\", state_tensor.shape)\n",
    "\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state_tensor)\n",
    "            return np.argmax(q_values.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        # Ensure state and next_state are float tensors\n",
    "        state = pad_or_truncate(state, max_length=512)\n",
    "        state = state.float()\n",
    "        next_state = next_state.float()\n",
    "\n",
    "        # Reshape next_state if not in correct shape\n",
    "        if next_state.ndim == 2 and next_state.shape[1] != 512:\n",
    "            # Handle reshaping here, maybe pad or truncate\n",
    "            next_state = pad_or_truncate(next_state, max_length=512)\n",
    "\n",
    "        # Ensure the tensors are of the correct shape\n",
    "        # print(\"Update: Shape of next_state_tensor:\", next_state.shape)\n",
    "        \n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        done = torch.tensor(done, dtype=torch.float)\n",
    "\n",
    "        q_update = reward + (self.gamma * torch.max(self.model(next_state)) * (1 - done))\n",
    "        q_values = self.model(state)\n",
    "        q_values[0][action] = q_update\n",
    "\n",
    "        loss = F.mse_loss(q_values, self.model(state))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "            # Gradient Clipping\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "def environment_step(model, input_ids, action):\n",
    "    # Append the action (token) to the input sequence\n",
    "    input_ids = torch.cat((input_ids, torch.tensor([[action]])), dim=1)\n",
    "\n",
    "    # Generate output from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get the next token (you might want a different approach here)\n",
    "    next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
    "\n",
    "    # Define a reward mechanism (this is highly task-specific)\n",
    "    reward = compute_reward(input_ids, next_token_id)\n",
    "\n",
    "    # Check if the end of sequence token is generated\n",
    "    done = next_token_id.item() == tokenizer.eos_token_id\n",
    "\n",
    "    return input_ids, reward, done\n",
    "\n",
    "def compute_perplexity(sequence):\n",
    "    \"\"\" Compute the perplexity of the generated sequence.\n",
    "    :param sequence: The generated text sequence.\n",
    "    :return: A numerical value representing the perplexity.\n",
    "    \"\"\"\n",
    "    # Tokenize the sequence\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate output from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "    # Perplexity is e^loss\n",
    "    perplexity = torch.exp(outputs.loss)\n",
    "    # Normalize perplexity to a smaller range, e.g., [0, 1]\n",
    "    return 1 / (1 + np.log(perplexity.item() + 1))\n",
    "\n",
    "def compute_reward(input_ids, next_token_id, target_context=None):\n",
    "    \"\"\" Compute the reward for the generated sequence.\n",
    "    :param input_ids: Tensor of input token IDs.\n",
    "    :param next_token_id: Tensor of the next token ID generated.\n",
    "    :param target_context: Optional context or topic that the generated text should align with.\n",
    "    :return: A numerical reward value. \"\"\"\n",
    "\n",
    "    # Convert token IDs to text\n",
    "    input_ids = input_ids.flatten()\n",
    "    # Check if token_id is a tensor and if so, convert it to a Python integer\n",
    "    next_token_id = next_token_id.item() if isinstance(next_token_id, torch.Tensor) else next_token_id\n",
    "    sequence = tokenizer.decode(input_ids.tolist() + [next_token_id])\n",
    "\n",
    "    # Fluency reward\n",
    "    fluency_reward = compute_fluency(sequence)\n",
    "\n",
    "    # Relevance reward (if target_context is provided)\n",
    "    relevance_reward = 0\n",
    "    if target_context:\n",
    "        relevance_reward = compute_relevance(sequence, target_context)\n",
    "\n",
    "    # Diversity reward\n",
    "    diversity_reward = diversity(sequence)\n",
    "\n",
    "    # Perplexity reward\n",
    "    perplexity_reward = -compute_perplexity(sequence)  # We negate it because lower perplexity is better\n",
    "\n",
    "    # Aggregate reward\n",
    "    # Normalize rewards\n",
    "    fluency_reward = normalize_reward(compute_fluency(sequence))\n",
    "    relevance_reward = normalize_reward(compute_relevance(sequence, target_context)) if target_context else 0\n",
    "    diversity_reward = normalize_reward(diversity(sequence))\n",
    "    perplexity_reward = normalize_reward(-compute_perplexity(sequence))\n",
    "\n",
    "    # Weighted sum of rewards\n",
    "    weights = {'fluency': 0.4, 'relevance': 0.3, 'diversity': 0.2, 'perplexity': 0.1}\n",
    "    total_reward = (weights['fluency'] * fluency_reward + \n",
    "                    weights['relevance'] * relevance_reward +\n",
    "                    weights['diversity'] * diversity_reward + \n",
    "                    weights['perplexity'] * perplexity_reward)\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def normalize_reward(reward, min_reward=-1, max_reward=1):\n",
    "    # Normalize reward to be within [min_reward, max_reward]\n",
    "    return (reward - min_reward) / (max_reward - min_reward)\n",
    "\n",
    "\n",
    "\n",
    "def compute_fluency(sequence):\n",
    "    \"\"\"\n",
    "    Compute a basic fluency score based on sentence length and structure.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(sequence)\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    # Average words per sentence - expecting a range around typical English sentence length (15-20 words)\n",
    "    avg_sentence_length = total_words / num_sentences if num_sentences > 0 else 0\n",
    "    \n",
    "    # Basic fluency score calculation\n",
    "    if 15 <= avg_sentence_length <= 20:\n",
    "        fluency_score = 1  # Ideal range\n",
    "    elif 10 <= avg_sentence_length < 15 or 20 < avg_sentence_length <= 25:\n",
    "        fluency_score = 0.75  # Acceptable range\n",
    "    else:\n",
    "        fluency_score = 0.5  # Less fluent\n",
    "\n",
    "    return fluency_score\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def compute_repetition_penalty(sequence):\n",
    "    \"\"\"\n",
    "    Compute a penalty for repetitive sequences.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(sequence)\n",
    "    sentence_set = set(sentences)\n",
    "    num_repeated_sentences = len(sentences) - len(sentence_set)\n",
    "    \n",
    "    # Penalty based on the proportion of repeated sentences\n",
    "    return num_repeated_sentences / len(sentences) if len(sentences) > 0 else 0\n",
    "\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    \"\"\"\n",
    "    Compute lexical diversity as the ratio of unique words to total words.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words) if len(words) > 0 else 0\n",
    "\n",
    "def diversity(sequence):\n",
    "    \"\"\"\n",
    "    Compute the diversity of the generated sequence.\n",
    "    \"\"\"\n",
    "    return lexical_diversity(sequence)\n",
    "\n",
    "\n",
    "def compute_embeddings(text):\n",
    "    \"\"\"\n",
    "    Compute BERT embeddings for the given text.\n",
    "    \"\"\"\n",
    "    inputs = bert_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)  # Average pooling\n",
    "\n",
    "def compute_relevance(sequence, target_context):\n",
    "    \"\"\"\n",
    "    Compute semantic similarity (relevance) between sequence and target context.\n",
    "    \"\"\"\n",
    "    sequence_embedding = compute_embeddings(sequence)\n",
    "    context_embedding = compute_embeddings(target_context)\n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity(sequence_embedding, context_embedding)\n",
    "\n",
    "    return cosine_similarity.item()\n",
    "\n",
    "# # Example usage\n",
    "# sequence = \"Advancements in AI technology have revolutionized many industries.\"\n",
    "# target_context = \"technology and innovation\"\n",
    "# relevance_score = compute_relevance(sequence, target_context)\n",
    "# print(\"Relevance Score:\", relevance_score)\n",
    "\n",
    "class OrcaDQN:\n",
    "    def __init__(self, model, dqn_agent, tokenizer):\n",
    "        self.model = model\n",
    "        self.dqn_agent = dqn_agent\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate_sequence(self, prompt):\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "        generated_sequence = []\n",
    "        while not self.end_condition_met(input_ids):\n",
    "            action = self.dqn_agent.select_action(input_ids)\n",
    "            input_ids, reward = environment_step(self.model, input_ids, action)\n",
    "            self.dqn_agent.update(input_ids, action, reward, input_ids)\n",
    "            generated_sequence.append(action)\n",
    "        return self.tokenizer.decode(generated_sequence)\n",
    "\n",
    "    def end_condition_met(self, input_ids, max_length=50):\n",
    "        # Stop if the EOS token is generated or max length is reached\n",
    "        return (input_ids[0][-1] == tokenizer.eos_token_id) or (input_ids.size(1) > max_length)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(orca_dqn, dqn_agent, num_episodes, target_context):\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
    "        input_ids = tokenizer.encode(target_context, return_tensors='pt')\n",
    "        # input_ids = input_ids.view(512, 128)\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            action = dqn_agent.select_action(input_ids)\n",
    "            next_input_ids, reward, done = environment_step(orca_dqn.model, input_ids, action)\n",
    "            dqn_agent.update(input_ids, action, reward, next_input_ids, done)\n",
    "\n",
    "            input_ids = next_input_ids\n",
    "            total_reward += reward\n",
    "\n",
    "            print(f\"Action: {action}, Reward: {reward}, Total Reward: {total_reward}\")\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode + 1} Complete. Total Reward: {total_reward}\")\n",
    "                print(f\"Generated Sequence: {orca_dqn.generate_sequence(target_context)}\\n\")\n",
    "\n",
    "def pad_or_truncate(sequence, max_length=512, pad_token_id=0):\n",
    "    # Ensure the sequence is 1D and pad or truncate to max_length\n",
    "    sequence = sequence.view(-1)\n",
    "    sequence_length = sequence.size(0)\n",
    "    if sequence_length > max_length:\n",
    "        return sequence[:max_length].unsqueeze(0)\n",
    "    elif sequence_length < max_length:\n",
    "        padding = torch.full((max_length - sequence_length,), pad_token_id, dtype=sequence.dtype)\n",
    "        return torch.cat((sequence, padding), dim=0).unsqueeze(0)\n",
    "    else:\n",
    "        return sequence.unsqueeze(0)\n",
    "\n",
    "# Initialize the DQN Agent\n",
    "state_size = 512  # This should match your model's input size\n",
    "action_size = tokenizer.vocab_size  # Total number of possible actions (tokens)\n",
    "hidden_size = 128  # This can be adjusted\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99\n",
    "target_context = \"Your target context here\" \n",
    "\n",
    "dqn_agent = DQNAgent(state_size, action_size, hidden_size, learning_rate, gamma)\n",
    "\n",
    "# Create the OrcaDQN instance\n",
    "orca_dqn = OrcaDQN(model, dqn_agent, tokenizer)\n",
    "\n",
    " # Define this if you are using it in compute_reward\n",
    "train(orca_dqn, dqn_agent, num_episodes=100, target_context=target_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU support) is not available in PyTorch.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU support) is available in PyTorch!\")\n",
    "    print(\"Number of GPU devices available:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA (GPU support) is not available in PyTorch.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
