{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:29<00:00,  9.67s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"fblgit/juanako-7b-UNA\"\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # device_map=\"cuda:0\"\n",
    ")\n",
    "# Create the tokenizer from the model object\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# print(llm(\"AI is going to\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 3094, Reward: -200556.53692054749, Total Reward: -200556.53692054749\n",
      "Action: 22733, Reward: -340540.7070055008, Total Reward: -541097.2439260483\n",
      "Action: 12931, Reward: -314730.5969352722, Total Reward: -855827.8408613205\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Network forward: Input shape:\", x.shape)  # Should be [1, 512]\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate, gamma):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQNNetwork(state_size, action_size, hidden_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # print(\"select_action: Original shape of state:\", state.shape)\n",
    "        state = state.float()\n",
    "        state_tensor = pad_or_truncate(state, max_length=512)\n",
    "        # # If state is a 2D tensor of shape [1, 512], no need to average across the last dimension\n",
    "        # if state.ndim == 2 and state.shape[1] == 512:\n",
    "        #     state_tensor = state\n",
    "        # elif state.ndim == 2:\n",
    "        #     # If state is a 2D tensor but not of the correct size, pad or truncate\n",
    "        #     state_tensor = pad_or_truncate(state, max_length=512)\n",
    "        # else:\n",
    "        #     # If state is not a 2D tensor, compute the embeddings\n",
    "        #     state_tensor = pad_or_truncate(state, max_length=512)\n",
    "        state_tensor = state_tensor.unsqueeze(0)  # Ensure it has a batch dimension\n",
    "        # print(\"select_action: Shape of state_tensor before passing to the network:\", state_tensor.shape)\n",
    "\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state_tensor)\n",
    "            return np.argmax(q_values.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        # Ensure state and next_state are float tensors\n",
    "        state = pad_or_truncate(state, max_length=512)\n",
    "        state = state.float()\n",
    "        next_state = next_state.float()\n",
    "\n",
    "        # Reshape next_state if not in correct shape\n",
    "        if next_state.ndim == 2 and next_state.shape[1] != 512:\n",
    "            # Handle reshaping here, maybe pad or truncate\n",
    "            next_state = pad_or_truncate(next_state, max_length=512)\n",
    "\n",
    "        # Ensure the tensors are of the correct shape\n",
    "        # print(\"Update: Shape of next_state_tensor:\", next_state.shape)\n",
    "        \n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        done = torch.tensor(done, dtype=torch.float)\n",
    "\n",
    "        q_update = reward + (self.gamma * torch.max(self.model(next_state)) * (1 - done))\n",
    "        q_values = self.model(state)\n",
    "        q_values[0][action] = q_update\n",
    "\n",
    "        loss = F.mse_loss(q_values, self.model(state))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "def environment_step(model, input_ids, action):\n",
    "    # Append the action (token) to the input sequence\n",
    "    input_ids = torch.cat((input_ids, torch.tensor([[action]])), dim=1)\n",
    "\n",
    "    # Generate output from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get the next token (you might want a different approach here)\n",
    "    next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
    "\n",
    "    # Define a reward mechanism (this is highly task-specific)\n",
    "    reward = compute_reward(input_ids, next_token_id)\n",
    "\n",
    "    # Check if the end of sequence token is generated\n",
    "    done = next_token_id.item() == tokenizer.eos_token_id\n",
    "\n",
    "    return input_ids, reward, done\n",
    "\n",
    "def compute_perplexity(sequence):\n",
    "    \"\"\" Compute the perplexity of the generated sequence.\n",
    "    :param sequence: The generated text sequence.\n",
    "    :return: A numerical value representing the perplexity.\n",
    "    \"\"\"\n",
    "    # Tokenize the sequence\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate output from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "    # Perplexity is e^loss\n",
    "    perplexity = torch.exp(outputs.loss)\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "def compute_reward(input_ids, next_token_id, target_context=None):\n",
    "    \"\"\" Compute the reward for the generated sequence.\n",
    "    :param input_ids: Tensor of input token IDs.\n",
    "    :param next_token_id: Tensor of the next token ID generated.\n",
    "    :param target_context: Optional context or topic that the generated text should align with.\n",
    "    :return: A numerical reward value.\n",
    "    \"\"\"\n",
    "    # Convert token IDs to text\n",
    "    sequence = tokenizer.decode(input_ids[0])\n",
    "\n",
    "    # Fluency reward\n",
    "    fluency_reward = compute_fluency(sequence)\n",
    "\n",
    "    # Relevance reward (if target_context is provided)\n",
    "    relevance_reward = 0\n",
    "    if target_context:\n",
    "        relevance_reward = compute_relevance(sequence, target_context)\n",
    "\n",
    "    # Diversity reward\n",
    "    diversity_reward = diversity(sequence)\n",
    "\n",
    "    # Perplexity reward\n",
    "    perplexity_reward = -compute_perplexity(sequence)  # We negate it because lower perplexity is better\n",
    "\n",
    "    # Aggregate reward\n",
    "    total_reward = fluency_reward + relevance_reward + diversity_reward + perplexity_reward\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def compute_fluency(sequence):\n",
    "    \"\"\" Compute the fluency of the generated sequence.\n",
    "    :param sequence: The generated text sequence.\n",
    "    :return: A numerical value representing the fluency.\n",
    "    \"\"\"\n",
    "    # Tokenize the sequence\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate output from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "    # The loss represents the fluency: lower loss means more fluent\n",
    "    fluency = -outputs.loss.item()\n",
    "\n",
    "    return fluency\n",
    "\n",
    "def diversity(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "def compute_embeddings(text):\n",
    "    \"\"\"\n",
    "    Compute the embeddings for the given text using the pre-trained model.\n",
    "    \n",
    "    :param text: A string or a list of strings to encode.\n",
    "    :return: A tensor of embeddings.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # If outputs is a tuple, the embeddings might be the first element\n",
    "    if isinstance(outputs, tuple):\n",
    "        embeddings = outputs[0]\n",
    "    else:\n",
    "        embeddings = outputs.logits  # or outputs.hidden_states depending on the model\n",
    "    return embeddings.mean(dim=1)\n",
    "\n",
    "\n",
    "def compute_relevance(sequence, target_context):\n",
    "    \"\"\"\n",
    "    Compute the relevance of the sequence to the target context using semantic similarity.\n",
    "\n",
    "    :param sequence: The generated text sequence.\n",
    "    :param target_context: The target context or topic for comparison.\n",
    "    :return: A numerical value representing the relevance.\n",
    "    \"\"\"\n",
    "    # Generate embeddings\n",
    "    sequence_embedding = compute_embeddings(sequence)\n",
    "    context_embedding = compute_embeddings(target_context)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_scores = torch.nn.functional.cosine_similarity(sequence_embedding, context_embedding)\n",
    "\n",
    "    # Extract the similarity score as a float\n",
    "    similarity_score = cosine_scores.item()\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# # Example usage\n",
    "# sequence = \"Advancements in AI technology have revolutionized many industries.\"\n",
    "# target_context = \"technology and innovation\"\n",
    "# relevance_score = compute_relevance(sequence, target_context)\n",
    "# print(\"Relevance Score:\", relevance_score)\n",
    "\n",
    "class OrcaDQN:\n",
    "    def __init__(self, model, dqn_agent, tokenizer):\n",
    "        self.model = model\n",
    "        self.dqn_agent = dqn_agent\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate_sequence(self, prompt):\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "        generated_sequence = []\n",
    "        while not self.end_condition_met(input_ids):\n",
    "            action = self.dqn_agent.select_action(input_ids)\n",
    "            input_ids, reward = environment_step(self.model, input_ids, action)\n",
    "            self.dqn_agent.update(input_ids, action, reward, input_ids)\n",
    "            generated_sequence.append(action)\n",
    "        return self.tokenizer.decode(generated_sequence)\n",
    "\n",
    "    def end_condition_met(self, input_ids, max_length=50):\n",
    "        # Stop if the EOS token is generated or max length is reached\n",
    "        return (input_ids[0][-1] == tokenizer.eos_token_id) or (input_ids.size(1) > max_length)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(orca_dqn, dqn_agent, num_episodes, target_context):\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
    "        input_ids = tokenizer.encode(target_context, return_tensors='pt')\n",
    "        # input_ids = input_ids.view(512, 128)\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            action = dqn_agent.select_action(input_ids)\n",
    "            next_input_ids, reward, done = environment_step(orca_dqn.model, input_ids, action)\n",
    "            dqn_agent.update(input_ids, action, reward, next_input_ids, done)\n",
    "\n",
    "            input_ids = next_input_ids\n",
    "            total_reward += reward\n",
    "\n",
    "            print(f\"Action: {action}, Reward: {reward}, Total Reward: {total_reward}\")\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode + 1} Complete. Total Reward: {total_reward}\")\n",
    "                print(f\"Generated Sequence: {orca_dqn.generate_sequence(target_context)}\\n\")\n",
    "\n",
    "def pad_or_truncate(sequence, max_length=512, pad_token_id=0):\n",
    "    # Ensure the sequence is 1D and pad or truncate to max_length\n",
    "    sequence = sequence.view(-1)\n",
    "    sequence_length = sequence.size(0)\n",
    "    if sequence_length > max_length:\n",
    "        return sequence[:max_length].unsqueeze(0)\n",
    "    elif sequence_length < max_length:\n",
    "        padding = torch.full((max_length - sequence_length,), pad_token_id, dtype=sequence.dtype)\n",
    "        return torch.cat((sequence, padding), dim=0).unsqueeze(0)\n",
    "    else:\n",
    "        return sequence.unsqueeze(0)\n",
    "\n",
    "\n",
    "# Initialize the DQN Agent\n",
    "state_size = 512  # This should match your model's input size\n",
    "action_size = tokenizer.vocab_size  # Total number of possible actions (tokens)\n",
    "hidden_size = 128  # This can be adjusted\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99\n",
    "target_context = \"Your target context here\" \n",
    "\n",
    "\n",
    "dqn_agent = DQNAgent(state_size, action_size, hidden_size, learning_rate, gamma)\n",
    "\n",
    "# Create the OrcaDQN instance\n",
    "orca_dqn = OrcaDQN(model, dqn_agent, tokenizer)\n",
    "\n",
    " # Define this if you are using it in compute_reward\n",
    "train(orca_dqn, dqn_agent, num_episodes=100, target_context=target_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU support) is not available in PyTorch.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU support) is available in PyTorch!\")\n",
    "    print(\"Number of GPU devices available:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA (GPU support) is not available in PyTorch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate, gamma):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQNNetwork(state_size, action_size, hidden_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0)\n",
    "                q_values = self.model(state)\n",
    "            return np.argmax(q_values.cpu().detach().numpy())\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        reward = torch.tensor(reward)\n",
    "        done = torch.tensor(done)\n",
    "\n",
    "        q_update = reward + (self.gamma * torch.max(self.model(next_state)) * (1 - done))\n",
    "        q_values = self.model(state)\n",
    "        q_values[0][action] = q_update\n",
    "\n",
    "        loss = F.mse_loss(q_values, self.model(state))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "def environment_step(model, input_ids, action):\n",
    "    # Append the action (token) to the input sequence\n",
    "    input_ids = torch.cat((input_ids, torch.tensor([[action]])), dim=1)\n",
    "\n",
    "    # Generate output from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get the next token (you might want a different approach here)\n",
    "    next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
    "\n",
    "    # Define a reward mechanism (this is highly task-specific)\n",
    "    reward = compute_reward(input_ids, next_token_id)\n",
    "\n",
    "    # Check if the end of sequence token is generated\n",
    "    done = next_token_id.item() == tokenizer.eos_token_id\n",
    "\n",
    "    return input_ids, reward, done\n",
    "\n",
    "def compute_reward(input_ids, next_token_id):\n",
    "    # Implement reward calculation\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Orca-2-13b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Orca-2-13b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrcaDQN:\n",
    "    def __init__(self, model, dqn_agent, tokenizer):\n",
    "        self.model = model\n",
    "        self.dqn_agent = dqn_agent\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate_sequence(self, prompt):\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "        generated_sequence = []\n",
    "        while not self.end_condition_met(input_ids):\n",
    "            action = self.dqn_agent.select_action(input_ids)\n",
    "            input_ids, reward = environment_step(self.model, input_ids, action)\n",
    "            self.dqn_agent.update(input_ids, action, reward, input_ids)\n",
    "            generated_sequence.append(action)\n",
    "        return self.tokenizer.decode(generated_sequence)\n",
    "\n",
    "    def end_condition_met(self, input_ids):\n",
    "        # Define the end condition for sequence generation\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, agent, num_episodes):\n",
    "    for episode in range(num_episodes):\n",
    "        # Start with an initial prompt\n",
    "        prompt = \"The quick brown fox \"\n",
    "        \n",
    "        # Generate a sequence\n",
    "        sequence = model.generate_sequence(prompt)\n",
    "        \n",
    "        # Evaluate the sequence and update the model (this part is highly task-dependent)\n",
    "        # ...\n",
    "\n",
    "# Initialize the DQN Agent\n",
    "dqn_agent = DQNAgent()\n",
    "\n",
    "# Create the OrcaDQN instance\n",
    "orca_dqn = OrcaDQN(model, dqn_agent, tokenizer)\n",
    "\n",
    "# Train the model\n",
    "train(orca_dqn, dqn_agent, num_episodes=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
