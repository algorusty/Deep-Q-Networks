{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.20s/it]\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wesle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"fblgit/juanako-7b-UNA\"\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    # device_map=\"cuda:0\"\n",
    ")\n",
    "# Create the tokenizer from the model object\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# print(llm(\"AI is going to\"))\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: <s> the quick brown fox jumps over buchan\n",
      "Fluency: 0.054166666666666696, Relevance: 0.0, Diversity: 1.0, Perplexity: -0.13848591339324035, Total: 0.05784037663671318\n",
      "Action: tensor([22699]), Reward: 0.05784037663671318, Total Reward: 0.05784037663671318\n",
      "Sequence: <s> the quick brown fox jumps over buch거n\n",
      "Fluency: 0.05384615384615388, Relevance: 0.0, Diversity: 1.0, Perplexity: -0.11847582534427681, Total: 0.06768516425093854\n",
      "Action: tensor([30013]), Reward: 0.06768516425093854, Total Reward: 0.12552554088765172\n",
      "Sequence: <s> the quick brown fox jumps over buch거ils\n",
      "\n",
      "Fluency: 0.050000000000000044, Relevance: 0.0, Diversity: 1.0, Perplexity: -0.11905108012302612, Total: 0.06547445993848697\n",
      "Action: tensor([4544]), Reward: 0.06547445993848697, Total Reward: 0.1910000008261387\n",
      "Sequence: <s> the quick brown fox jumps over buch거ils underarter\n",
      "\n",
      "Fluency: 0.050000000000000044, Relevance: 0.0, Diversity: 1.0, Perplexity: -0.1089907549327751, Total: 0.07050462253361248\n",
      "Action: tensor([12872]), Reward: 0.07050462253361248, Total Reward: 0.26150462335975116\n",
      "Sequence: <s> the quick brown fox jumps over buch거ils underarter调查\n",
      "Fluency: 0.05312500000000009, Relevance: 0.0, Diversity: 1.0, Perplexity: -0.10581769214414694, Total: 0.07365365392792658\n",
      "Action: tensor([29231]), Reward: 0.07365365392792658, Total Reward: 0.3351582772876778\n",
      "Sequence: <s> the quick brown fox jumps over buch거ils underarter调alex\n",
      "Fluency: 0.05294117647058827, Relevance: 0.0, Diversity: 1.0, Perplexity: -0.10165907663156204, Total: 0.07564104991951312\n",
      "Action: tensor([282]), Reward: 0.07564104991951312, Total Reward: 0.4107993272071909\n",
      "Sequence: <s> the quick brown fox jumps over buch거ils underarter调al Heavenly\n",
      "Fluency: 0.05277777777777781, Relevance: 0.0, Diversity: 1.0, Perplexity: -0.09948048261466202, Total: 0.0766486475815579\n",
      "Action: tensor([22830]), Reward: 0.0766486475815579, Total Reward: 0.4874479747887488\n",
      "Sequence: <s> the quick brown fox jumps over buch거ils underarter调al Heavencourt \n",
      "Fluency: 0.050000000000000044, Relevance: 0.0, Diversity: 0.981637513409912, Perplexity: -0.10370558675508912, Total: 0.07131095796344666\n",
      "Action: tensor([28129]), Reward: 0.07131095796344666, Total Reward: 0.5587589327521955\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size, learning_rate, gamma):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = DQNNetwork(state_size, action_size, hidden_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = state.float()\n",
    "        state_tensor = pad_or_truncate(state, max_length=512)\n",
    "        state_tensor = state_tensor.unsqueeze(0)  # Ensure it has a batch dimension\n",
    "\n",
    "        if random.random() <= self.epsilon:\n",
    "            return torch.tensor([random.randrange(self.action_size)], dtype=torch.long)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state_tensor)\n",
    "            return torch.tensor([np.argmax(q_values.cpu().detach().numpy())], dtype=torch.long)\n",
    "\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = pad_or_truncate(state, max_length=512).float()\n",
    "        next_state = pad_or_truncate(next_state, max_length=512).float()\n",
    "\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        done = torch.tensor(done, dtype=torch.float)\n",
    "\n",
    "        # Compute Q values for current and next state\n",
    "        q_values = self.model(state)\n",
    "        q_next = self.model(next_state).detach()\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        q_update = reward + self.gamma * q_next.max(1)[0] * (1 - done)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(q_values.gather(1, action.unsqueeze(1)), q_update.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Epsilon decay\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "def environment_step(model, input_ids, action):\n",
    "    input_ids = torch.cat((input_ids, torch.tensor([[action]])), dim=1)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "    logits = outputs.logits\n",
    "    next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
    "    reward = compute_reward(input_ids, next_token_id)\n",
    "    done = next_token_id.item() == tokenizer.eos_token_id\n",
    "    return input_ids, reward, done\n",
    "\n",
    "def compute_perplexity(sequence):\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    # Perplexity is e^loss\n",
    "    perplexity = torch.exp(outputs.loss)\n",
    "    return 1 / (1 + np.log(perplexity.item() + 1))\n",
    "\n",
    "def compute_reward(input_ids, next_token_id, target_context_embedding=None):\n",
    "    \"\"\" Compute the reward for the generated sequence. \"\"\"\n",
    "    input_ids = input_ids.flatten()\n",
    "    next_token_id = next_token_id.item() if isinstance(next_token_id, torch.Tensor) else next_token_id\n",
    "    sequence_tokens = input_ids.tolist() + [next_token_id]\n",
    "    \n",
    "    # Decode the sequence tokens into a sentence and print it\n",
    "    sequence_sentence = tokenizer.decode(sequence_tokens)\n",
    "    print(f\"Sequence: {sequence_sentence}\")\n",
    "    \n",
    "    fluency_reward = compute_fluency(sequence_tokens)\n",
    "    relevance_reward = 0\n",
    "    if target_context_embedding is not None:\n",
    "        sequence_embedding = compute_embeddings(tokenizer.decode(sequence_tokens))\n",
    "        relevance_reward = compute_relevance(sequence_embedding, target_context_embedding)\n",
    "    diversity_reward = lexical_diversity(sequence_tokens)\n",
    "    perplexity_reward = -compute_perplexity(tokenizer.decode(sequence_tokens))\n",
    "    fluency_reward = normalize_reward(fluency_reward)\n",
    "    relevance_reward = normalize_reward(relevance_reward)\n",
    "    diversity_reward = normalize_reward(diversity_reward)\n",
    "    perplexity_reward = normalize_reward(perplexity_reward)\n",
    "    weights = {'fluency': 0.5, 'relevance': 0.1, 'diversity': 0.1, 'perplexity': 0.7}\n",
    "    total_reward = (weights['fluency'] * fluency_reward + \n",
    "                    weights['relevance'] * relevance_reward +\n",
    "                    weights['diversity'] * diversity_reward + \n",
    "                    weights['perplexity'] * perplexity_reward)\n",
    "    print(f\"Fluency: {fluency_reward}, Relevance: {relevance_reward}, Diversity: {diversity_reward}, Perplexity: {perplexity_reward}, Total: {total_reward}\")\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "def normalize_reward(reward, min_reward=-1, max_reward=1):\n",
    "    # Normalize reward to be within [min_reward, max_reward]\n",
    "    return (reward - min_reward) / (max_reward - min_reward) * 2 - 1\n",
    "\n",
    "def compute_fluency(tokens):\n",
    "    sentences = [tokenizer.decode(sentence_tokens) for sentence_tokens in tokens]\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    num_sentences = len(sentences)\n",
    "    avg_sentence_length = total_words / num_sentences if num_sentences > 0 else 0\n",
    "\n",
    "    # Define the ideal sentence length\n",
    "    ideal_sentence_length = 20\n",
    "\n",
    "    # Calculate the fluency score based on how close the average sentence length is to the ideal length\n",
    "    fluency_score = 1 - abs(avg_sentence_length - ideal_sentence_length) / ideal_sentence_length\n",
    "\n",
    "    # Ensure the fluency score is between 0 and 1\n",
    "    fluency_score = max(0, min(fluency_score, 1))\n",
    "\n",
    "    return fluency_score\n",
    "\n",
    "import math\n",
    "\n",
    "def lexical_diversity(tokens):\n",
    "    words = [tokenizer.decode(token) for token in tokens]\n",
    "    unique_words = set(words)\n",
    "    num_types = len(unique_words)\n",
    "    num_tokens = len(words)\n",
    "    \n",
    "    if num_tokens > 0 and num_types > 0:\n",
    "        herdan_c = math.log(num_types) / math.log(num_tokens)\n",
    "    else:\n",
    "        herdan_c = 0\n",
    "\n",
    "    return herdan_c\n",
    "\n",
    "def compute_relevance(sequence_embedding, target_context_embedding):\n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity(sequence_embedding, target_context_embedding)\n",
    "    return cosine_similarity.item()\n",
    "\n",
    "def compute_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "class LLMDQN:\n",
    "    def __init__(self, model, dqn_agent, tokenizer):\n",
    "        self.model = model\n",
    "        self.dqn_agent = dqn_agent\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate_sequence(self, prompt):\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
    "        generated_sequence = []\n",
    "        while not self.end_condition_met(input_ids):\n",
    "            action = self.dqn_agent.select_action(input_ids)\n",
    "            input_ids, reward = environment_step(self.model, input_ids, action)\n",
    "            self.dqn_agent.update(input_ids, action, reward, input_ids)\n",
    "            generated_sequence.append(action)\n",
    "        return self.tokenizer.decode(generated_sequence)\n",
    "\n",
    "    def end_condition_met(self, input_ids, max_length=50):\n",
    "        return (input_ids[-1] == tokenizer.eos_token_id) or (input_ids.size(1) > max_length)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(llm_dqn, dqn_agent, num_episodes, target_context):\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
    "        input_ids = tokenizer.encode(target_context, return_tensors='pt')\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = dqn_agent.select_action(input_ids)\n",
    "            next_input_ids, reward, done = environment_step(llm_dqn.model, input_ids, action)\n",
    "            dqn_agent.update(input_ids, action, reward, next_input_ids, done)\n",
    "            input_ids = next_input_ids\n",
    "            total_reward += reward\n",
    "            print(f\"Action: {action}, Reward: {reward}, Total Reward: {total_reward}\")\n",
    "            if done:\n",
    "                print(f\"Episode {episode + 1} Complete. Total Reward: {total_reward}\")\n",
    "                print(f\"Generated Sequence: {llm_dqn.generate_sequence(target_context)}\\n\")\n",
    "\n",
    "\n",
    "def pad_or_truncate(sequence, max_length=512, pad_token_id=0):\n",
    "    sequence = sequence.view(-1)\n",
    "    sequence_length = sequence.size(0)\n",
    "    if sequence_length > max_length:\n",
    "        return sequence[:max_length].unsqueeze(0)\n",
    "    elif sequence_length < max_length:\n",
    "        padding = torch.full((max_length - sequence_length,), pad_token_id, dtype=sequence.dtype)\n",
    "        return torch.cat((sequence, padding), dim=0).unsqueeze(0)\n",
    "    else:\n",
    "        return sequence.unsqueeze(0)\n",
    "\n",
    "state_size = 512\n",
    "action_size = tokenizer.vocab_size\n",
    "hidden_size = 128\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99\n",
    "target_context = \"the quick brown fox jumps over \"\n",
    "\n",
    "dqn_agent = DQNAgent(state_size, action_size, hidden_size, learning_rate, gamma)\n",
    "llm_dqn = LLMDQN(model, dqn_agent, tokenizer)\n",
    "\n",
    "train(llm_dqn, dqn_agent, num_episodes=100, target_context=target_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU support) is not available in PyTorch.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU support) is available in PyTorch!\")\n",
    "    print(\"Number of GPU devices available:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA (GPU support) is not available in PyTorch.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
